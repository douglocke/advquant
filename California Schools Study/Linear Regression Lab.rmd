---
title: "Linear Regression Lab"
output:
  pdf_document: default
  html_notebook: default
---
This is the R Notebook version of the Linear Regression lab. This verison of the document allows for R code and text to coexist in the same script. The R script version of this file, which is how we operated the labs, is also available in canvas. You can learn more about R Notebook at this tutorial if you would like to utilize this functionality: https://rmarkdown.rstudio.com/r_notebooks.html

The first task when we open RStudio is to set our working directory. As covered in both of the optional labs, and in the documentation, navigate in the files window (bottom right) to your documents folder for this class. In "More", click "Set as Working Directory".

Now let's load in the dataset. You can either open it from your working directory (if you've saved it there) or pull it up with the below command. Change the file path to make sure it matches yours. 

```{r}
calschooldist <- read.csv("calschooldist.csv")
View(calschooldist)
```

We can rename our dataset as an object. You can name this whatever you want, but I recommend calschool because it will be in line with the code below.

```{r}
calschool <- calschooldist
```

We need to load "packages" in R which include commands and functions that we can leverage. These packages will vary based on the lab. Today we are going to be using the below for the assignment. You only need to do this once:

```{r, eval=FALSE, include=FALSE}
install.packages("car")
install.packages("psych")
```

Activate these packages: You need to do this every time you open R. We've only installed them, now we need to activate them for this instance. 

```{r}
library(car)  
library(psych)
```

If you're having trouble installing the car package, try the following code:

```{r, eval=FALSE, include=FALSE}
install.packages("lme4")
packageurl <- "https://cran.r-project.org/src/contrib/Archive/pbkrtest/pbkrtest_0.4-4.tar.gz" 
install.packages(packageurl, repos=NULL, type="source")
install.packages("car")
library("car")
```

To see our numbers (particularly the p values) more clearly, we can remove scientific notation. We need to repeat this every time we open R.  

```{r}
options(scipen=999)
```

Now that our working directory is set, our packages are loaded, we can begin the steps outlined in the prompt.

STEP 1: no R code to complete this

STEP 2: no R code to complete this

STEP 3: no R code to complete this

STEP 4: Perform basic checks of the candidate variables. Do you have any missing value or out of range data problems? If so, what did you do to resolve them, if anything?
First, let's look at plots to get a feel for the data. In R, we can just use "plot" to create a scatterplot of two variables. We have to define those two items. Let's plot our dependent variable, acadperf, against some of the independent variables. 

```{r}
plot(calschool$acadperf,calschool$meals)
plot(calschool$acadperf,calschool$ell)
plot(calschool$acadperf,calschool$yr_rnd)
plot(calschool$acadperf,calschool$hsg)
```

Have you noticed that any have a strong correlation? What does the graph of acadperf vs ell tell us about the ell data? Confirm this with the str code we completed in the lab. 
We can try a new function here, called "describe". This will give us new statistics that are a little different than "summary". 

```{r}
describe(calschool) 
```

Here we can see missing values, ranges, measures of central tendency, and standard deviation. What are your inferences about these values? I can see that most variables include 400 total entries, but mobility and acs have less than 400. I have to remove the nulls with the following code:   

```{r}
calschooldist2=na.omit(calschooldist)
describe(calschooldist2)
```

Now `calschooldist2` does not include any null values. Lets replace calschool so we don't have these NA values. 

```{r}
calschool <- calschooldist2
```

Step 5: What did your check of the correlation matrix find? Did you add any variables to the end of you list based on it? Does it look like you need to worry about multicollinearity? 
Do you remember this from the lab? This is a function that gives us not only our Pearson's Coefficients, but also gives us our p-values too! This function will be loaded into R, then when we run cor.prob with our data, we'll get the output.
When looking at output generated by cor.prob, remember, p values are above and coefficients are below the diagonal line. 

```{r}
cor.prob <- function (X, dfr = nrow(X) - 2) {
  R <- cor(X, use="pairwise.complete.obs")
  above <- row(R) < col(R)
  r2 <- R[above]^2
  Fstat <- r2 * dfr/(1 - r2)
  R[above] <- 1 - pf(Fstat, 1, dfr)
  R[row(R) == col(R)] <- NA
  R
}
```

Now that we have added the function, let's run it 

```{r}
correlation_table_calschool <- cor.prob(calschool)
View(correlation_table_calschool)
```

You can save this table to your working directory with the following code:

```{r}
write.csv(correlation_table_calschool, file = "Correlation Matrix California Schools.csv")
```

Are there any noteworthy correlations that might help you build your model? 
Which variables have the strongest relationships with academic performance?
Would these variables be good to include in a regression analysis?

STEP 6: no R code to complete this

STEP 7: REGRESSION! Add your first independent variable. Show your bivariate / unadjusted model. Did it accord with your expectations? 
Now we'll see a new command, "lm". This will fit a simple regression model to the data. The format for this code is lm(y~x, data) where y is the response (dependent variable), x is the predictor (independent variable), and the data is calschool.  
Give it a try with your first variable, but YOU MUST CHANGE the variable name where I have [your first variable]:

```{r, eval=FALSE, include=FALSE}
regression_1 <- lm(acadperf ~ [your first variable], data = calschool)
```

This is what it would look like if "meals" was your first variable:

```{r}
regression_1 <- lm(acadperf ~ meals, data = calschool)
```

Let's review the summary of this regression:

```{r}
summary(regression_1) 
```

This tells us the results of our regression. Is it in line with your expectations?

STEP 8: Check for regression violations for this bivariate mode. Did you find any major violations? 
To analyze residuals and test assumptions, we can explore some graphs. We'll start with a histogram of the residuals.

```{r}
hist(scale(regression_1$residuals))
```

Let's tell R to layout our graphs in a matrix so we can easily view 4 graphs at once. 
The plot() function graphs 4 helpful plots for us. 

```{r}
layout(matrix(c(1,2,3,4),2,2))
plot(regression_1)
```

What if we want standardized coefficients? R is a little difficult in that it doesn't give them as part of the standard output. 
By using "scale" in our lm function, we can standardize the unit of analysis to compare coefficients. Input your first variable.

```{r, eval=FALSE, include=FALSE}
lm(scale(acadperf) ~ scale([your first variable]), data = calschool)
```

Mine will look like this:

```{r}
lm(scale(acadperf) ~ scale(meals), data = calschool)
```

How can the standardized coefficient be interpreted?
How about standardized residuals? We'll start by viewing all residuals. R has a function "names()" so we can learn more about what information is available. 

```{r}
names(regression_1)
```

You can see that we have access to the coefficients and residuals of this regression. This can be an easy way to look only at the relevant data. 

```{r}
coefficients(regression_1)
```

We can also specify the entire set of residuals of each point (residual = predicted value - actual value)

```{r}
regression_1$residuals 
```

Next, we'll create a component of our regression to store our standardized residuals. We'll use the standardized residuals to determine if any have an absolute value greater than 2.  

```{r}
regression_1$standardized.residuals <- rstandard(regression_1)
regression_1$large_residual <- regression_1$standardized.residuals >2 | regression_1$standardized.residuals < -2
sum(regression_1$large_residual) 
```

What is your interpretation of the results?
Let's calculate the Durban-Watson statistic.

```{r}
dwt(regression_1)
```

STEP 9: Sequentially build up the model adding variables in the order you specified (don't check reg. assumptions at each stage) 
To build variables into your model, continue to use the lm() function, and add the variable on the back side of the ~. You should update the name of the model so you can save each iteration in R. The form is below. Don't forget to replace with your variables in the order that you defined.

```{r, eval=FALSE, include=FALSE}
regression_2 <- lm(acadperf ~ YOUR_VARIABLE_1 + YOUR_VARIABLE_2, data = calschool)

regression_3 <- lm(acadperf ~ YOUR_VARIABLE_1 + YOUR_VARIABLE_2 + YOUR_VARIABLE_3, data = calschool)

regression_4 <- lm(acadperf ~ YOUR_VARIABLE_1 + YOUR_VARIABLE_2 + YOUR_VARIABLE_3 + YOUR_VARIABLE_4, data = calschool)

regression_5 <- lm(acadperf ~ YOUR_VARIABLE_1 + YOUR_VARIABLE_2 + YOUR_VARIABLE_3 + YOUR_VARIABLE_4 + YOUR_VARIABLE_5, data = calschool)

regression_final <- lm(acadperf ~ YOUR_VARIABLE_1 + YOUR_VARIABLE_2 + YOUR_VARIABLE_3 + YOUR_VARIABLE_4 + YOUR_VARIABLE_5 + YOUR_VARIABLE_6, data = calschool)
summary(regression_final)
```

As an example, this is the form:

```{r}
regression_final <- lm(acadperf ~ meals + hsg + some_col, data = calschool)
summary(regression_final)
```

STEP 10: Recheck model assumptions
Once we have our final model, let's check assumptions through residual analysis as we did earlier.

```{r}
hist(scale(regression_final$residuals))
```

View all plots at once

```{r}
layout(matrix(c(1,2,3,4),2,2))
plot(regression_final)
```

Standardized residual analysis (how many residuals are more than two deviations away)

```{r}
regression_final$standardized.residuals <- rstandard(regression_final)
regression_final$large_residual <- regression_final$standardized.residuals >2 | regression_final$standardized.residuals < -2
sum(regression_final$large_residual)
```

Calculate the standardized coefficients as we did prior.

```{r, eval=FALSE, include=FALSE}
lm(scale(acadperf) ~ scale(YOUR_VARIABLE_1) + scale(YOUR_VARIABLE_2) + scale(YOUR_VARIABLE_3) + scale(YOUR_VARIABLE_4) + scale(YOUR_VARIABLE_5) + scale(YOUR_VARIABLE_6), data = calschool) 
```

Mine will look like this:

```{r}
lm(scale(acadperf) ~ scale(meals) + scale(hsg) + scale(some_col), data = calschool) 
```

You may have more (or fewer) variables in your model in comparison to my examples and that is okay. Just make sure all your variables in your final model are in standardized coefficients. 
Now that we have multiple terms in the model, lets include the multicollinearity test as well as Durbin-Watson. 

```{r}
vif(regression_final)
dwt(regression_final)
```

Discuss these results in your report and/or technical appendix. Don't forget to include an advanced extension, in a different file. 
